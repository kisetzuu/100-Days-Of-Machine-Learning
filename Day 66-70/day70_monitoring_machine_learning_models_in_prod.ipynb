{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "import prometheus_client as prom\n",
    "from flask import Flask, request, jsonify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Initial Model Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Prometheus metrics\n",
    "model_accuracy = prom.Gauge('model_accuracy', 'Accuracy of the deployed model')\n",
    "prediction_latency = prom.Summary('prediction_latency', 'Latency of predictions')\n",
    "\n",
    "# Update accuracy metric\n",
    "model_accuracy.set(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Parse input features\n",
    "    input_data = request.json['features']\n",
    "    input_array = np.array(input_data).reshape(1, -1)\n",
    "\n",
    "    # Make prediction\n",
    "    prediction = model.predict(input_array)\n",
    "    class_name = iris.target_names[prediction[0]]\n",
    "\n",
    "    # Measure latency\n",
    "    latency = time.time() - start_time\n",
    "    prediction_latency.observe(latency)\n",
    "\n",
    "    return jsonify({\"predicted_class\": class_name, \"latency\": latency})\n",
    "\n",
    "@app.route('/metrics')\n",
    "def metrics():\n",
    "    return prom.generate_latest(prom.REGISTRY)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    prom.start_http_server(8000)  # Start Prometheus on port 8000\n",
    "    app.run(debug=True, port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python app.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -X POST -H \"Content-Type: application/json\" -d '{\"features\": [5.1, 3.5, 1.4, 0.2]}' http://127.0.0.1:5000/predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://127.0.0.1:8000/metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudo apt-get install grafana\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# Simulate new data with drift\n",
    "new_data = X_test + np.random.normal(0, 0.5, X_test.shape)\n",
    "\n",
    "# Detect drift using Kolmogorov-Smirnov test\n",
    "for i, feature in enumerate(iris.feature_names):\n",
    "    stat, p_value = ks_2samp(X_test[:, i], new_data[:, i])\n",
    "    if p_value < 0.05:\n",
    "        print(f\"Feature '{feature}' shows significant drift (p-value: {p_value:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups:\n",
    "  - name: model_monitoring\n",
    "    rules:\n",
    "    - alert: AccuracyDrop\n",
    "      expr: model_accuracy < 0.8\n",
    "      for: 2m\n",
    "      labels:\n",
    "        severity: warning\n",
    "      annotations:\n",
    "        summary: \"Model accuracy dropped below 80%\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
