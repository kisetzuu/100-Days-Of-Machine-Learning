{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a sample model (e.g., Random Forest) for which we’ll analyze feature importance using SHAP and LIME.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load Iris dataset\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SHAP to understand feature importance for the entire model (global) and individual predictions (local).\n",
    "\n",
    "import shap\n",
    "\n",
    "# Initialize SHAP explainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Calculate SHAP values for the test set\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Plot summary of feature importance for all classes\n",
    "shap.summary_plot(shap_values, X_test, feature_names=data.feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SHAP’s force plot to interpret individual predictions and see how features contribute to specific outcomes.\n",
    "\n",
    "# Select a single instance for local interpretation\n",
    "instance_index = 0\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][instance_index], X_test[instance_index], feature_names=data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LIME to interpret individual predictions by approximating the model locally with a simpler interpretable model.\n",
    "\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "# Initialize LIME explainer\n",
    "lime_explainer = LimeTabularExplainer(X_train, feature_names=data.feature_names, class_names=data.target_names, discretize_continuous=True)\n",
    "\n",
    "# Select a single instance for explanation\n",
    "exp = lime_explainer.explain_instance(X_test[instance_index], model.predict_proba)\n",
    "\n",
    "# Show LIME explanation\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SHAP and LIME explanations to see how each method provides unique insights into the model’s predictions.\n",
    "\n",
    "# SHAP provides both global and local explainability, ideal for understanding the model as a whole and individual instances.\n",
    "# LIME focuses on local interpretability by approximating the model with a simpler one, highlighting influential features for single predictions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
