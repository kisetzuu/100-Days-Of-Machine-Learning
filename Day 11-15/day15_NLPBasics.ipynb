{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess text data by removing noise, which is crucial for building NLP models.\n",
    "\n",
    "import re\n",
    "\n",
    "# Example text data\n",
    "text = \"Hello World! Welcome to NLP. ðŸ˜Š This is Day 15: #ML100Days\"\n",
    "\n",
    "# Convert to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# Remove punctuation and special characters\n",
    "text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# Remove numbers\n",
    "text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "print(\"Preprocessed Text:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove common words (stopwords) that do not contribute to the meaning of text.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Filter out stopwords\n",
    "tokens = [word for word in tokens if word not in stop_words]\n",
    "print(\"Tokens without Stopwords:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove common words (stopwords) that do not contribute to the meaning of text.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Filter out stopwords\n",
    "tokens = [word for word in tokens if word not in stop_words]\n",
    "print(\"Tokens without Stopwords:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce words to their root form using stemming or lemmatization to treat similar words as one.\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Apply stemming and lemmatization\n",
    "stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "print(\"Stemmed Words:\", stemmed_words)\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent text as a Bag of Words (BoW), which transforms text data into numerical features.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example corpus\n",
    "corpus = [\"This is the first document.\", \"This document is the second document.\", \"And this is the third one.\"]\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Show BoW representation\n",
    "print(\"Bag of Words Representation:\\n\", X.toarray())\n",
    "print(\"Feature Names:\", vectorizer.get_feature_names_out())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
