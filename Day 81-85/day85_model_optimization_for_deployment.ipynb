{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install onnx onnxruntime tensorflow tensorflow-model-optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow_model_optimization.sparsity.keras import prune_low_magnitude\n",
    "import onnx\n",
    "import onnxruntime as ort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize data\n",
    "\n",
    "# Define a simple model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile and train the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=3, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to TensorFlow Lite format with quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Enable default optimizations\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the quantized model\n",
    "with open(\"model_quantized.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"Quantized model saved as model_quantized.tflite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "\n",
    "# Apply pruning to the model\n",
    "pruning_params = {'pruning_schedule': tf.keras.experimental.PruningSchedule.PolynomialDecay(initial_sparsity=0.2, final_sparsity=0.8, begin_step=0, end_step=1000)}\n",
    "pruned_model = prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# Compile and retrain the pruned model\n",
    "pruned_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "pruned_model.fit(x_train, y_train, epochs=2, validation_data=(x_test, y_test))\n",
    "\n",
    "# Strip pruning wrappers for deployment\n",
    "pruned_model = strip_pruning(pruned_model)\n",
    "\n",
    "# Save the pruned model\n",
    "pruned_model.save(\"model_pruned.h5\")\n",
    "print(\"Pruned model saved as model_pruned.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "\n",
    "# Convert the model to ONNX format\n",
    "onnx_model = tf2onnx.convert.from_keras(model)\n",
    "\n",
    "# Save the ONNX model\n",
    "with open(\"model.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(\"Model converted to ONNX and saved as model.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the original model\n",
    "original_loss, original_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Original Model Accuracy: {original_accuracy:.4f}\")\n",
    "\n",
    "# Load and evaluate the quantized model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"model_quantized.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Prepare the input data\n",
    "interpreter.set_tensor(input_details[0]['index'], x_test[:1].astype('float32'))\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "print(f\"Quantized Model Output: {output_data}\")\n",
    "\n",
    "# Evaluate the pruned model\n",
    "pruned_loss, pruned_accuracy = pruned_model.evaluate(x_test, y_test)\n",
    "print(f\"Pruned Model Accuracy: {pruned_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ONNX model for inference\n",
    "session = ort.InferenceSession(\"model.onnx\")\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "# Perform inference\n",
    "result = session.run([output_name], {input_name: x_test[:1].reshape(1, 28, 28).astype('float32')})\n",
    "print(f\"ONNX Model Prediction: {result}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
