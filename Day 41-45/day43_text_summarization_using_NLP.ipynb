{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample text document or article for summarization.\n",
    "\n",
    "text = \"\"\"\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on the interaction \n",
    "between humans and computers using natural language. NLP techniques are widely used in various applications \n",
    "such as chatbots, translation, text summarization, and sentiment analysis. Text summarization is a crucial \n",
    "application of NLP that allows users to condense lengthy documents into shorter, coherent versions while \n",
    "retaining the essential information. Summarization techniques can be categorized into two types: extractive \n",
    "and abstractive. Extractive summarization involves selecting the most important sentences from the original \n",
    "text, whereas abstractive summarization generates new sentences that convey the core meaning. Extractive \n",
    "methods are easier to implement as they rely on scoring and ranking sentences based on their importance.\n",
    "\"\"\"\n",
    "\n",
    "# Print original text\n",
    "print(\"Original Text:\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the text: tokenize sentences and words, and remove stopwords.\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "\n",
    "# Download resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Tokenize text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Tokenize and clean words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    words = [word for word in words if word not in stop_words and word not in string.punctuation]\n",
    "    return words\n",
    "\n",
    "# Cleaned words\n",
    "cleaned_words = preprocess_text(text)\n",
    "print(\"\\nCleaned Words Sample:\", cleaned_words[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the frequency of each word in the cleaned text.\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Word frequency\n",
    "word_frequencies = Counter(cleaned_words)\n",
    "\n",
    "# Normalize word frequencies\n",
    "max_freq = max(word_frequencies.values())\n",
    "for word in word_frequencies:\n",
    "    word_frequencies[word] = word_frequencies[word] / max_freq\n",
    "\n",
    "print(\"\\nNormalized Word Frequencies:\")\n",
    "print(dict(list(word_frequencies.items())[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score sentences based on the sum of word frequencies of words they contain.\n",
    "\n",
    "sentence_scores = {}\n",
    "for sentence in sentences:\n",
    "    words = preprocess_text(sentence)\n",
    "    for word in words:\n",
    "        if word in word_frequencies:\n",
    "            if sentence not in sentence_scores:\n",
    "                sentence_scores[sentence] = word_frequencies[word]\n",
    "            else:\n",
    "                sentence_scores[sentence] += word_frequencies[word]\n",
    "\n",
    "# Print top scored sentences\n",
    "print(\"\\nTop Scored Sentences:\")\n",
    "print(sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the top N scored sentences to form the summary.\n",
    "\n",
    "def extract_summary(sentence_scores, num_sentences=3):\n",
    "    top_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)[:num_sentences]\n",
    "    summary = \" \".join([sentence for sentence, score in top_sentences])\n",
    "    return summary\n",
    "\n",
    "# Generate summary\n",
    "summary = extract_summary(sentence_scores, num_sentences=3)\n",
    "print(\"\\nGenerated Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the summary with the original text.\n",
    "\n",
    "print(\"\\nOriginal Text Length:\", len(text.split()))\n",
    "print(\"Summary Length:\", len(summary.split()))\n",
    "print(\"\\nSummary:\\n\", summary)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
