{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast future values\n",
    "future_days = 10\n",
    "future_predictions = []\n",
    "\n",
    "# Use the last available sequence\n",
    "current_input = X_test[-1]\n",
    "\n",
    "for _ in range(future_days):\n",
    "    next_prediction = model.predict(current_input.reshape(1, SEQ_LENGTH, 1))[0]\n",
    "    future_predictions.append(next_prediction)\n",
    "    current_input = np.append(current_input[1:], next_prediction).reshape(SEQ_LENGTH, 1)\n",
    "\n",
    "# Inverse transform the predictions\n",
    "future_predictions = scaler.inverse_transform(future_predictions)\n",
    "\n",
    "# Display predictions\n",
    "print(f\"Next {future_days} days predicted prices:\")\n",
    "print(future_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Input, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract the Flickr8k dataset (images + captions)\n",
    "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
    "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
    "!unzip Flickr8k_Dataset.zip\n",
    "!unzip Flickr8k_text.zip\n",
    "\n",
    "# Define paths\n",
    "image_folder = 'Flickr8k_Dataset/Flicker8k_Dataset'\n",
    "caption_file = 'Flickr8k_text/Flickr8k.token.txt'\n",
    "\n",
    "# Load and display the caption file\n",
    "captions = open(caption_file, 'r').read()\n",
    "print(\"Sample Captions:\\n\", captions.split(\"\\n\")[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Parse captions and map image names to their captions\n",
    "def load_captions(caption_file):\n",
    "    captions_dict = {}\n",
    "    for line in captions.split(\"\\n\"):\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        image_id, caption = line.split(\"\\t\")\n",
    "        image_id = image_id.split(\"#\")[0]\n",
    "        caption = re.sub(r\"[^a-zA-Z0-9 ]\", \"\", caption.lower())\n",
    "        if image_id not in captions_dict:\n",
    "            captions_dict[image_id] = []\n",
    "        captions_dict[image_id].append(\"startseq \" + caption + \" endseq\")\n",
    "    return captions_dict\n",
    "\n",
    "captions_dict = load_captions(caption_file)\n",
    "print(\"Sample Processed Captions:\", captions_dict[list(captions_dict.keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG16 model for image feature extraction\n",
    "base_model = VGG16(weights='imagenet')\n",
    "cnn_model = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
    "\n",
    "def extract_features(image_path):\n",
    "    image = load_img(image_path, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = tf.keras.applications.vgg16.preprocess_input(image)\n",
    "    features = cnn_model.predict(image)\n",
    "    return features.flatten()\n",
    "\n",
    "# Extract features for one sample image\n",
    "image_path = os.path.join(image_folder, list(captions_dict.keys())[0])\n",
    "features = extract_features(image_path)\n",
    "print(\"Extracted Features Shape:\", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize captions\n",
    "all_captions = [caption for captions_list in captions_dict.values() for caption in captions_list]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_captions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(\"Vocabulary Size:\", vocab_size)\n",
    "\n",
    "# Prepare sequences\n",
    "def create_sequences(tokenizer, captions, max_length):\n",
    "    X_text, y_text = [], []\n",
    "    for caption in captions:\n",
    "        seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "        for i in range(1, len(seq)):\n",
    "            X_text.append(seq[:i])\n",
    "            y_text.append(seq[i])\n",
    "    X_text = pad_sequences(X_text, maxlen=max_length, padding='post')\n",
    "    y_text = np.array(y_text)\n",
    "    return X_text, y_text\n",
    "\n",
    "max_length = max(len(caption.split()) for caption in all_captions)\n",
    "X_text, y_text = create_sequences(tokenizer, all_captions, max_length)\n",
    "print(\"Text Sequences Shape:\", X_text.shape, y_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Image Captioning model\n",
    "embedding_dim = 256\n",
    "\n",
    "# Image feature input\n",
    "image_input = Input(shape=(4096,))\n",
    "image_features = Dense(256, activation='relu')(image_input)\n",
    "\n",
    "# Caption input\n",
    "text_input = Input(shape=(max_length,))\n",
    "text_features = Embedding(vocab_size, embedding_dim, mask_zero=True)(text_input)\n",
    "text_features = LSTM(256)(text_features)\n",
    "\n",
    "# Combine both inputs\n",
    "decoder = tf.keras.layers.add([image_features, text_features])\n",
    "output = Dense(vocab_size, activation='softmax')(decoder)\n",
    "\n",
    "# Compile the model\n",
    "model = Model(inputs=[image_input, text_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for training (requires feature extraction for all images)\n",
    "# image_features_dict = {image_id: extract_features(image_folder + '/' + image_id) for image_id in captions_dict.keys()}\n",
    "\n",
    "# Uncomment this code when features are ready\n",
    "# X_image = np.array([image_features_dict[image_id] for image_id in captions_dict.keys()])\n",
    "# model.fit([X_image, X_text], y_text, epochs=20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate caption for an image\n",
    "def generate_caption(model, tokenizer, image_features, max_length):\n",
    "    caption = 'startseq'\n",
    "    for _ in range(max_length):\n",
    "        seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "        seq = pad_sequences([seq], maxlen=max_length, padding='post')\n",
    "        y_pred = np.argmax(model.predict([image_features, seq]), axis=-1)\n",
    "        word = tokenizer.index_word.get(y_pred[0], '')\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "        caption += ' ' + word\n",
    "    return caption.replace('startseq', '').replace('endseq', '').strip()\n",
    "\n",
    "# Test the model on a new image\n",
    "# print(\"Generated Caption:\", generate_caption(model, tokenizer, features, max_length))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
