{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning Example: Solving a Gridworld Problem\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define the environment\n",
    "states = 5  # States: 0, 1, 2, 3, 4\n",
    "actions = 2  # Actions: Left (0), Right (1)\n",
    "rewards = [-1, -1, -1, -1, 10]  # Rewards for each state\n",
    "\n",
    "# Initialize Q-table\n",
    "Q = np.zeros((states, actions))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "episodes = 1000\n",
    "\n",
    "# Q-Learning algorithm\n",
    "for _ in range(episodes):\n",
    "    state = 0  # Start at the first state\n",
    "    while state != states - 1:\n",
    "        action = np.random.choice(actions)\n",
    "        next_state = state + 1 if action == 1 else max(state - 1, 0)\n",
    "        Q[state, action] += alpha * (rewards[next_state] + gamma * np.max(Q[next_state]) - Q[state, action])\n",
    "        state = next_state\n",
    "\n",
    "print(\"Trained Q-Table:\\n\", Q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the environment for Q-Learning (Gridworld Example)\n",
    "import numpy as np\n",
    "\n",
    "# Define the states and actions\n",
    "states = 5  # States: 0 to 4\n",
    "actions = ['left', 'right']\n",
    "\n",
    "# Define the reward matrix\n",
    "rewards = np.array([\n",
    "    [0, 1],  # State 0: reward for 'left' and 'right'\n",
    "    [0, 1],  # State 1\n",
    "    [0, 1],  # State 2\n",
    "    [0, 1],  # State 3\n",
    "    [0, 10]  # State 4: terminal state with high reward\n",
    "])\n",
    "\n",
    "# Transition probabilities (deterministic in this case)\n",
    "transitions = {\n",
    "    0: {'left': 0, 'right': 1},\n",
    "    1: {'left': 0, 'right': 2},\n",
    "    2: {'left': 1, 'right': 3},\n",
    "    3: {'left': 2, 'right': 4},\n",
    "    4: {'left': 4, 'right': 4}  # Terminal state\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table and hyperparameters\n",
    "q_table = np.zeros((states, len(actions)))  # Q-table for state-action pairs\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.2  # Exploration rate (epsilon-greedy strategy)\n",
    "episodes = 1000  # Number of episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table and hyperparameters\n",
    "q_table = np.zeros((states, len(actions)))  # Q-table for state-action pairs\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.2  # Exploration rate (epsilon-greedy strategy)\n",
    "episodes = 1000  # Number of episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the optimal policy\n",
    "optimal_policy = [actions[np.argmax(q_table[state])] for state in range(states)]\n",
    "print(\"Optimal Policy:\", optimal_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Q-value updates (Optional)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example visualization for one state\n",
    "plt.plot(q_table[0, :], label=\"State 0\")\n",
    "plt.title(\"Q-Value Updates for State 0\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Q-Value\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
