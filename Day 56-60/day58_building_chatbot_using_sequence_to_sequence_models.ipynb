{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract the dataset\n",
    "!wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
    "!unzip cornell_movie_dialogs_corpus.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "lines = open(\"cornell movie-dialogs corpus/movie_lines.txt\", encoding=\"utf-8\", errors=\"ignore\").read().split(\"\\n\")\n",
    "conversations = open(\"cornell movie-dialogs corpus/movie_conversations.txt\", encoding=\"utf-8\", errors=\"ignore\").read().split(\"\\n\")\n",
    "\n",
    "# Extract pairs of questions and answers\n",
    "id_to_line = {}\n",
    "for line in lines:\n",
    "    parts = line.split(\" +++$+++ \")\n",
    "    if len(parts) == 5:\n",
    "        id_to_line[parts[0]] = parts[4]\n",
    "\n",
    "pairs = []\n",
    "for convo in conversations:\n",
    "    parts = convo.split(\" +++$+++ \")\n",
    "    if len(parts) == 4:\n",
    "        ids = eval(parts[3])\n",
    "        for i in range(len(ids) - 1):\n",
    "            pairs.append((id_to_line[ids[i]], id_to_line[ids[i + 1]]))\n",
    "\n",
    "# Display sample pairs\n",
    "print(\"\\nSample Question-Answer Pairs:\")\n",
    "for pair in pairs[:5]:\n",
    "    print(f\"Q: {pair[0]}\")\n",
    "    print(f\"A: {pair[1]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "questions = [clean_text(pair[0]) for pair in pairs]\n",
    "answers = [\"<start> \" + clean_text(pair[1]) + \" <end>\" for pair in pairs]\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(questions + answers)\n",
    "\n",
    "# Convert text to sequences\n",
    "questions_seq = tokenizer.texts_to_sequences(questions)\n",
    "answers_seq = tokenizer.texts_to_sequences(answers)\n",
    "\n",
    "# Pad sequences\n",
    "max_len = max(max(len(seq) for seq in questions_seq), max(len(seq) for seq in answers_seq))\n",
    "questions_seq = pad_sequences(questions_seq, maxlen=max_len, padding=\"post\")\n",
    "answers_seq = pad_sequences(answers_seq, maxlen=max_len, padding=\"post\")\n",
    "\n",
    "# Display tokenized samples\n",
    "print(\"\\nTokenized Question Sample:\", questions_seq[0])\n",
    "print(\"Tokenized Answer Sample:\", answers_seq[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoder\n",
    "encoder_inputs = Input(shape=(max_len,))\n",
    "encoder_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=256)(encoder_inputs)\n",
    "encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding)\n",
    "\n",
    "# Define attention mechanism\n",
    "decoder_inputs = Input(shape=(max_len,))\n",
    "decoder_embedding = Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=256)(decoder_inputs)\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[encoder_state_h, encoder_state_c])\n",
    "\n",
    "attention = tf.keras.layers.Attention()([decoder_outputs, encoder_outputs])\n",
    "decoder_concat_input = tf.concat([decoder_outputs, attention], axis=-1)\n",
    "decoder_dense = Dense(len(tokenizer.word_index) + 1, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_concat_input)\n",
    "\n",
    "# Build the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare decoder target data\n",
    "answers_target = np.zeros_like(answers_seq)\n",
    "answers_target[:, :-1] = answers_seq[:, 1:]\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    [questions_seq, answers_seq],\n",
    "    answers_target,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(\"chatbot_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = load_model(\"chatbot_model.h5\")\n",
    "\n",
    "# Generate responses\n",
    "def generate_response(input_text, model, tokenizer, max_len):\n",
    "    input_seq = tokenizer.texts_to_sequences([clean_text(input_text)])\n",
    "    input_seq = pad_sequences(input_seq, maxlen=max_len, padding=\"post\")\n",
    "    decoder_input = np.zeros((1, max_len))\n",
    "    decoder_input[0, 0] = tokenizer.word_index[\"<start>\"]\n",
    "\n",
    "    response = \"\"\n",
    "    for i in range(max_len - 1):\n",
    "        pred = model.predict([input_seq, decoder_input])\n",
    "        token = np.argmax(pred[0, i, :])\n",
    "        word = tokenizer.index_word.get(token, \"\")\n",
    "        if word == \"<end>\":\n",
    "            break\n",
    "        response += word + \" \"\n",
    "        decoder_input[0, i + 1] = token\n",
    "\n",
    "    return response.strip()\n",
    "\n",
    "# Test the chatbot\n",
    "test_question = \"How are you?\"\n",
    "response = generate_response(test_question, model, tokenizer, max_len)\n",
    "print(\"\\nChatbot Response:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
